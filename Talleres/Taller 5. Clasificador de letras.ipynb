{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Usando el dataset de iris cargue el conjunto de entrenamiento (X) y las etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('Datasets/dataFI.mat')\n",
    "data = np.array(list(data.values()))\n",
    "X=data[3]\n",
    "t=data[4]\n",
    "t= np.argmax(t, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Siga los siguientes pasos (2.5):\n",
    "\n",
    "* Use Normalizer para normalizar el conjunto de entrenamiento <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\">Ver</a>.\n",
    "* Use train_test_split para armar el conjunto de entrenamiento y el conjunto de pruebas. Use 70% para entrenar y 30% para pruebas <a href=\"https://scikit-learn.org/stable/modules/cross_validation.html#\">Ver</a>.\n",
    "* Use una red neuronal que tenga como solver un gradiente descendiente estocástico (sgd) y como función de activación (activation) la función logística (logistic). Defina los demás hiperparámetros para ajustar el entrenamiento y realice el ajuste <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\">Ver</a>.\n",
    "* Prediga los resultados para t de entrenamiento y t de pruebas. Usando accuracy_score determine la exactitud para t de entrenamiento y t de pruebas <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\">Ver</a>. \n",
    "* ¿Qué resultados obtuvo? Ahora puede modificar los parámetros que desee de la red neuronal. ¿Qué resultados obtuvo? Interprete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funcion de activacion Sigmoidal con Gradiente Descendiente\n",
      "Exactitud Conjunto de entrenamiento: 0.38095238095238093\n",
      "Exactitud Conjunto de pruebas: 0.2222222222222222\n",
      "\n",
      "Funcion de activacion Tangente Hiperbolico con Solver Adam\n",
      "Exactitud Conjunto de entrenamiento: 0.9714285714285714\n",
      "Exactitud Conjunto de pruebas: 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "print(\"Funcion de activacion Sigmoidal con Gradiente Descendiente\")\n",
    "\n",
    "transformer = Normalizer().fit(X)\n",
    "Xn = transformer.transform(X) #Normalizamos X\n",
    "\n",
    "Xe, Xp, ye, yp = train_test_split(Xn, t, test_size=0.3)\n",
    "\n",
    "clf = MLPClassifier(activation='logistic', solver='sgd', max_iter=20000)\n",
    "clf.fit(Xe, ye)\n",
    "\n",
    "pe = clf.predict(Xe)\n",
    "exacte = accuracy_score(ye, pe) #Conjunto de entrenamiento\n",
    "print(\"Exactitud Conjunto de entrenamiento: \" + str(exacte)) \n",
    "\n",
    "pp = clf.predict(Xp)\n",
    "exactp = accuracy_score(yp, pp) #Conjunto de pruebas\n",
    "print(\"Exactitud Conjunto de pruebas: \" + str(exactp))\n",
    "\n",
    "print(\"\\nFuncion de activacion Tangente Hiperbolico con Solver Adam\")\n",
    "\n",
    "transformer = Normalizer().fit(X)\n",
    "Xn = transformer.transform(X) #Normalizamos X\n",
    "\n",
    "Xe, Xp, ye, yp = train_test_split(Xn, t, test_size=0.3)\n",
    "\n",
    "clf = MLPClassifier(activation='tanh', alpha=0.0002, max_iter=7500)\n",
    "clf.fit(Xe, ye)\n",
    "\n",
    "pe = clf.predict(Xe)\n",
    "exacte = accuracy_score(ye, pe) #Conjunto de entrenamiento\n",
    "print(\"Exactitud Conjunto de entrenamiento: \" + str(exacte))\n",
    "\n",
    "pp = clf.predict(Xp)\n",
    "exactp = accuracy_score(yp, pp) #Conjunto de pruebas\n",
    "print(\"Exactitud Conjunto de pruebas: \" + str(exactp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretacion:\n",
    "Funcion de activacion Sigmoidal con Gradiente Descendiente:\n",
    "No resulto muy efectivo la funcion sigmoidal, puede que resulte con sobreaprendizae o los congjuntos, pero en fin resulta mas eficiente el tanjente. esto se deve a que al aplicar tal cosa, converge a 0 o se aproxima. esto puede ser debido a\n",
    "MLPClassifier se entrena de forma iterativa, ya que en cada paso se calculan las derivadas parciales de la función de pérdida con respecto a los parámetros del modelo para actualizar los parámetros. este resultado pudo haber sido afectado por la escala entre datos, que aunque se halla normalizado aun puede presentar una escala no factible.\n",
    "\n",
    "\n",
    "Funcion de activacion Tangente Hiperbolico con Solver Adam\n",
    "la funcion tangente resulto siendo mas efectivo en este caso que el sigmoidal. esto es devido a sus configuraciones presenta una mejor deformidad de datos,menos esfuerzo matematicamente y con ello mejor aprendizaje. claro esto no siempre se va a reflejar en todos los casos, pero en este resulta eficaz, o almenos mejor respecto al sigmoidal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Cargue la base de datos y divida las etiquetas del conjunto de entrenamiento. Transforme las etiquetas (Letras) a números (0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Datasets/letter-recognition.data', names=np.arange(17)).to_numpy()\n",
    "X=data[:,1:16]\n",
    "t=data[:,0]\n",
    "\n",
    "for i, c in enumerate(t):\n",
    "    t[i] = ord(c) - 65\n",
    "    \n",
    "X=X.astype('int')\n",
    "t=t.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Siga los mismos pasos del punto 2, pero ahora en lugar de usar train_test_split use cross_val_score con un cv = 7 (<a href=\"https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics\">Ver</a>). ¿Qué resultados obtuvo? Interprete (2.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funcion de activacion Sigmoidal con Gradiente Descendiente\n",
      "Exactitud: 0.6897\n",
      "Funcion de activacion Tangente Hiperbolica con Gradiente Descendiente\n",
      "Exactitud: 0.9144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshu\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "print(\"Funcion de activacion Sigmoidal con Gradiente Descendiente\")\n",
    "\n",
    "transformer = Normalizer().fit(X)\n",
    "Xn = transformer.transform(X)\n",
    "\n",
    "clf = MLPClassifier(activation='logistic', solver='sgd', max_iter=5000)#cuantas iteraciones\n",
    "clf.fit(Xn, t)\n",
    "\n",
    "scores = cross_val_score(clf, Xn, t, cv=5)\n",
    "exact = np.mean(scores)\n",
    "print(\"Exactitud: \" + str(exact))\n",
    "\n",
    "print(\"Funcion de activacion Tangente Hiperbolica con Gradiente Descendiente\")\n",
    "transformer = Normalizer().fit(X)\n",
    "Xn = transformer.transform(X)\n",
    "\n",
    "clf = MLPClassifier(activation='tanh', max_iter=5000) # Modifica solo los hiperparametros de aqui\n",
    "clf.fit(Xn, t)\n",
    "\n",
    "scores = cross_val_score(clf, Xn, t, cv=4) # Tambien puedes modificar el cv\n",
    "exact = np.mean(scores)\n",
    "print(\"Exactitud: \" + str(exact)) # Tiene que tener mejor exactidud que el de arriba, no cambies nada mas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretacion: \n",
    "Funcion de activacion Sigmoidal con Gradiente Descendiente:\n",
    "Aunque obtuvo una exactitud mayor, claramente resulta menos eficiente que el otro metodo. ademas que al aplicar cross validation, realiza una iteraciones que resulta mas esfuerzo pa la maquina, haciendolo mas demorado y mas costoso computacionalmente. se necesita aplicar bien los valores de hiperparametros y otros ajustes, pero claramente es mejor usar otra funcion.\n",
    "\n",
    "Funcion de activacion Tangente Hiperbolica con Gradiente Descendiente:\n",
    "Como en el paso 1, su exactitud resulta siendo muy alta, aunque de igualmanera al realizar croos calidation, su costo aumenta y no presento mucho cambio, aunque presenta un apredizaje bien y buenos resultados. Considero mejor usar el paso . el conjunto de prueba puede sobreajustarse, por eso no siempre es la mejor solucion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
